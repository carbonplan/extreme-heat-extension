{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 06: Bias-correction\n",
    "*Develop a model that resolves differences between the climate model data and those from a more-detailed reference historical timeseries, and then use that model to ensure that future projections also reflect that level of detail.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coiled\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from utils import gcm_list, load_regions\n",
    "from xclim import sdba\n",
    "from xclim.sdba.adjustment import QuantileDeltaMapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=5,\n",
    "    worker_vm_types=[\"m7g.large\"],\n",
    "    scheduler_vm_types=[\"m7g.4xlarge\"],\n",
    "    region=\"us-west-2\",\n",
    "    spot_policy=\"spot_with_fallback\",\n",
    ")\n",
    "\n",
    "\n",
    "cluster.adapt(minimum=2, maximum=50)\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_projection(gcm, scenario):\n",
    "    \"\"\"\n",
    "    Load in a WBGT in the shade estimate produced by in `05_aggregate.ipynb`.\n",
    "    \"\"\"\n",
    "    ds = xr.open_zarr(\n",
    "        f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-regions/{gcm}-{scenario}.zarr\"\n",
    "    )\n",
    "    ds['time'] = ds['time'].dt.floor('D')    \n",
    "    ds[\"WBGT\"].attrs = {}\n",
    "    ds[\"WBGT\"].attrs[\"units\"] = \"degC\"\n",
    "    ds[\"processing_id\"] = ds[\"processing_id\"].astype(\"int\")\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load UHE-daily estimates developed in `05_aggregate.ipynb`. This data will be the reference for every bias-correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = xr.open_zarr(\n",
    "    \"s3://carbonplan-climate-impacts/extreme-heat/v1.1/inputs/wbgt-UHE-daily-historical.zarr\"\n",
    ")\n",
    "ref[\"WBGT\"].attrs[\"units\"] = \"degC\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Load the region information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regions_df = load_regions(extension=\"central-asia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bias_correction(ref_ts, model_ts):\n",
    "    \"\"\"\n",
    "    Prep timeseries for training and train the bias-correction model\n",
    "    \"\"\"    \n",
    "\n",
    "    # convert all ts to the no-leap calendar and convert back to\n",
    "    # gregorian after prediction\n",
    "    \n",
    "    ref_ts = ref_ts.convert_calendar(\"noleap\")\n",
    "    model_ts = model_ts.convert_calendar(\n",
    "        \"noleap\",\n",
    "        dim=\"time\",\n",
    "        align_on=\"year\",\n",
    "        missing=np.nan,\n",
    "        use_cftime=None,\n",
    "    )\n",
    "\n",
    "    # gap fill by linearly interpolating\n",
    "    model_ts = model_ts.interpolate_na(dim=\"time\", method=\"linear\").chunk({\"time\": -1})\n",
    "    group = sdba.Grouper(\"time.dayofyear\", window=31)\n",
    "    nquantiles = 100\n",
    "\n",
    "    # train the same model but using different groupers\n",
    "    trained_model = QuantileDeltaMapping.train(\n",
    "        ref_ts, model_ts, nquantiles=nquantiles, kind=\"+\", group=group\n",
    "    )\n",
    "\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bias_correction(trained_model, ts, out_store):\n",
    "    \"\"\"\n",
    "    Apply trained bias-correction model to each model timeseries (whether historic\n",
    "    or future).\n",
    "    \"\"\"\n",
    "    bias_corrected = trained_model.adjust(ts)\n",
    "\n",
    "    # the rolling monthly bias-correction\n",
    "    # works with no-leap calendars so convert it back to gregorian\n",
    "    bias_corrected = (\n",
    "        bias_corrected.convert_calendar(\n",
    "            \"gregorian\",\n",
    "            align_on=\"year\",\n",
    "            missing=np.nan,\n",
    "            use_cftime=None,\n",
    "        )\n",
    "        .interpolate_na(dim=\"time\", method=\"linear\")\n",
    "        .chunk({\"time\": -1})\n",
    "    )\n",
    "    bias_corrected.to_dataset().to_zarr(out_store,zarr_format=2, mode='w',consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Data isn't available for all regions. Only apply bias-correction where data is available in both the reference and the modelled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_places = ref.processing_id.values\n",
    "modelled_places = load_projection(\"ACCESS-CM2\", \"historical\")[\n",
    "    \"WBGT\"\n",
    "].processing_id.values\n",
    "valid_ids = list(set(ref_places) & set(modelled_places))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define the periods over which the bias-correction will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: for central asia ext added more analysis periods\n",
    "analysis_period_dict = {\n",
    "    \"historical\": slice(\"1985\", \"2014\"),\n",
    "    \"ssp245-2030\": slice(\"2020\", \"2039\"),\n",
    "    \"ssp245-2050\": slice(\"2040\", \"2059\"),\n",
    "    \"ssp245-2070\": slice(\"2060\", \"2079\"),\n",
    "    \"ssp245-2080\": slice(\"2070\", \"2089\"),\n",
    "    \"ssp245-2090\": slice(\"2080\", \"2099\"),\n",
    "    \"ssp370-2030\": slice(\"2020\", \"2039\"),\n",
    "    \"ssp370-2050\": slice(\"2040\", \"2059\"),\n",
    "    \"ssp370-2070\": slice(\"2060\", \"2079\"),\n",
    "    \"ssp370-2080\": slice(\"2070\", \"2089\"),\n",
    "    \"ssp370-2090\": slice(\"2080\", \"2099\"),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Subset the reference dataset to the historical time period used for training (1985-2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = (\n",
    "    ref.sel(time=analysis_period_dict[\"historical\"])\n",
    "    .sel(processing_id=valid_ids)\n",
    "    .chunk({\"time\": -1, \"processing_id\": 850})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Load in the different datasets into a dictionary, which, instead of an Xarray object, allows for the different calendars that different GCMs use. Then, for each GCM separately, train a bias-correction model and use it to bias-correct the historic and future projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "### UPDATE - SUBSET!\n",
    "for gcm in gcm_list[0:1]:\n",
    "    ts_dict = {}\n",
    "    ts_dict[\"reference\"] = ref[\"WBGT\"]\n",
    "    for scenario in analysis_period_dict.keys():\n",
    "        ts_dict[scenario] = load_projection(gcm, scenario.split(\"-\")[0])[\"WBGT\"]\n",
    "        ts_dict[scenario] = (\n",
    "            ts_dict[scenario]\n",
    "            .sel(processing_id=valid_ids)\n",
    "            .chunk({\"time\": -1, \"processing_id\": 850})\n",
    "        )\n",
    "        ts_dict[scenario] = ts_dict[scenario].sel(time=analysis_period_dict[scenario])\n",
    "        print(f'loaded: {scenario}')\n",
    "    trained_model = train_bias_correction(\n",
    "        ts_dict[\"reference\"], ts_dict[\"historical\"]\n",
    "    )\n",
    "\n",
    "    for scenario in analysis_period_dict.keys():\n",
    "        apply_bias_correction(\n",
    "            trained_model,\n",
    "            ts_dict[scenario],\n",
    "            f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-regions/{gcm}-{scenario}-bc.zarr\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extreme-heat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
