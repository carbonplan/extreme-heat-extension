{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 05: Climate data aggregation\n",
    "*Aggregate gridded WBGT in the shade estimates into region-averaged estimates. Do this for both the reference dataset (UHE-Daily) as well as the climate change projections developed by `02_generate.ipynb`. This code is based on the Pangeo post [Conservative Region Aggregation with Xarray, Geopandas and Sparse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/1) by Ryan Abernathey. Much of the functionality is from the [extended example](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/16) by Rich Signell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask import delayed\n",
    "from dask.distributed import progress\n",
    "from utils import gcm_list, load_regions, prep_sparse, spatial_aggregation, load_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-24 11:04:57,765][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-01-24 11:04:57,766][INFO    ][coiled.package_sync] Resolving your local extreme-heat Python environment...\n",
      "[2025-01-24 11:04:58,181][INFO    ][coiled.package_sync] Scanning 285 conda packages...\n",
      "[2025-01-24 11:04:58,183][INFO    ][coiled.package_sync] Scanning 154 python packages...\n",
      "[2025-01-24 11:04:58,397][INFO    ][coiled] Running pip check...\n",
      "[2025-01-24 11:04:58,795][INFO    ][coiled] Validating environment...\n",
      "[2025-01-24 11:05:00,705][INFO    ][coiled] Creating wheel for ~/Documents/carbonplan/extreme-heat-extension/central_asia/notebooks...\n",
      "[2025-01-24 11:05:00,810][INFO    ][coiled] Uploading coiled_local_notebooks...\n",
      "[2025-01-24 11:05:02,342][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-01-24 11:05:03,364][INFO    ][coiled] Creating Cluster (name: 05, https://cloud.coiled.io/clusters/739294?account=carbonplan ). This usually takes 1-2 minutes...\n",
      "2025-01-24 11:05:53,383 - distributed.deploy.adaptive - INFO - Adaptive scaling started: minimum=1 maximum=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-01-24 11:09:03,454][INFO    ][coiled] Adaptive scaling up to 2 workers.\n"
     ]
    }
   ],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=1,\n",
    "    name=\"05\",\n",
    "    worker_vm_types=[\"m7g.medium\"],\n",
    "    scheduler_vm_types=[\"c7g.large\"],\n",
    "    region=\"us-west-2\",\n",
    "    spot_policy=\"spot_with_fallback\",\n",
    ")\n",
    "\n",
    "cluster.adapt(minimum=1, maximum=100)\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Define functions for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_population(grid_name=\"CarbonPlan\"):\n",
    "    \"\"\"\n",
    "    Load the population data generated in `03_population.ipynb`.\n",
    "    \"\"\"\n",
    "    population_dict = {\n",
    "        \"CHC\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/\"\n",
    "        \"GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_UHE_daily.zarr\",\n",
    "        \"CarbonPlan\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/\"\n",
    "        \"inputs/GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_CP.zarr\",\n",
    "    }\n",
    "    population = xr.open_zarr(population_dict[grid_name])\n",
    "    population = population.rename({\"x\": \"lon\", \"y\": \"lat\"}).drop(\"spatial_ref\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac343d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(gcm: str, scenario: str, years: np.ndarray):\n",
    "    \"\"\"\n",
    "    Load in the gridded WBGT in the shade estimates from `02_generate.ipynb`.\n",
    "    \"\"\"\n",
    "    ds = xr.open_zarr(\n",
    "        f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-gridded/years/{gcm}/{gcm}-{scenario}.zarr\",\n",
    "    )\n",
    "    ds = ds.sel(time=slice(str(years[0]), str(years[-1])))\n",
    "    ds = ds.assign_coords(lon=(((ds[\"lon\"] + 180) % 360) - 180)).sortby(\"lon\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lon = \"lon\"\n",
    "lat = \"lat\"\n",
    "scenario_years = {\n",
    "    \"historical\": np.arange(1985, 2015),\n",
    "    \"ssp245\": np.arange(2015, 2100),\n",
    "    \"ssp370\": np.arange(2015, 2100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "regions_df = load_regions(extension=\"central-asia\")\n",
    "\n",
    "buffer = (\n",
    "    0.5  # padding to expand bounds to ensure you grab the data covering each region\n",
    ")\n",
    "bbox = tuple(\n",
    "    [\n",
    "        regions_df.total_bounds[0] - buffer,\n",
    "        regions_df.total_bounds[1] - buffer,\n",
    "        regions_df.total_bounds[2] + buffer,\n",
    "        regions_df.total_bounds[3] + buffer,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Access the gridded UHE-Daily data from Tuholske et al (2021) and extract timeseries for the regions of interest. These will form the reference dataset for `06_bias_correction.ipynb`. Thanks to Cascade Tuholske (Montana State University) and Pete Peterson (University of California, Santa Barbara) for making the gridded dataset available. The source gridded dataset may not remain available indefinitely, but the full city- and region-aggregated version is available here alongside the other inputs for the analysis, maintaining reproducibility of the project. \n",
    "\n",
    "The next steps aggregate the gridded datasets to region-average estimates. The non-city regions encompass all land area and thus sometimes include significant stretches of uninhabited land with potentially erroneously high or low temperatures (e.g., deserts). Weighting the aggregation by a gridded population product helps ensure that the estimates are human-relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Load the UHE-Daily dataset and calculate weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created from: https://github.com/carbonplan/uhe-daily-recipe\n",
    "ds = xr.open_zarr(\n",
    "    \"s3://carbonplan-climate-impacts/extreme-heat-extension/v1.0/inputs/uhe_daily_zarr_v2.zarr\",\n",
    "    consolidated=True,\n",
    "    chunks={},\n",
    ")\n",
    "ds\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "\n",
    "population = load_population(grid_name=\"CHC\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use weights to aggregate gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Repeat the above process but for our gridded WBGT estimates developed in `02_generate.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Load a sample dataset as a template to calculate weights. The same weights can be used for every projection because all GCMs are on the same 0.25 degree grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/7d7yq_4j2qgdfm_j3j4tsyl40000gn/T/ipykernel_1351/2101520248.py:12: DeprecationWarning: dropping variables using `drop` is deprecated; use drop_vars.\n",
      "  population = population.rename({\"x\": \"lon\", \"y\": \"lat\"}).drop(\"spatial_ref\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating weights...\n",
      "population_weights calculated\n"
     ]
    }
   ],
   "source": [
    "ds = load_ds(\"ACCESS-CM2\", \"historical\", np.arange(1985, 1986))  # noqa : F821\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "population = load_population(grid_name=\"CarbonPlan\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Aggregate all gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@delayed\n",
    "def region_avg_estimate(gcm_scenario_tuple: tuple, population: xr.Dataset) -> tuple:\n",
    "    gcm, scenario = gcm_scenario_tuple\n",
    "    ds = load_ds(gcm, scenario, scenario_years[scenario])  # noqa : F821\n",
    "    ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "    population = population.sel(\n",
    "        lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3])\n",
    "    )\n",
    "\n",
    "    regridded = spatial_aggregation(ds, sparse_weights, load=False)\n",
    "    regridded = regridded.chunk(chunks={\"time\": -1, \"processing_id\": 100})\n",
    "    fp = f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-regions/{gcm}-{scenario}_ori.zarr\"\n",
    "    regridded.to_zarr(fp, mode=\"w\", consolidated=True)\n",
    "    return gcm_scenario_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_scenario_tuples = [\n",
    "    (gcm, scenario)\n",
    "    for gcm in gcm_list\n",
    "    for scenario in [\"historical\", \"ssp245\", \"ssp370\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############## SUBSET TEMP #################\n",
    "delayed_results = []\n",
    "for gcm_scenario in [(\"ACCESS-CM2\", \"historical\")]:\n",
    "    result = region_avg_estimate(gcm_scenario, population)\n",
    "    delayed_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655bda2d0baa4e43bd1d556782f7da92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = dask.persist(delayed_results, retires=1)\n",
    "progress(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "<!-- half this took about ~16 minutes -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=1, maximum=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 11:14:37,986 - distributed.deploy.adaptive - INFO - Adaptive scaling stopped: minimum=1 maximum=100. Reason: unknown\n",
      "[2025-01-24 11:14:38,297][INFO    ][coiled] Cluster 739294 deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extreme-heat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
