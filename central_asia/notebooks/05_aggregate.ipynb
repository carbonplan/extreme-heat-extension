{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 05: Climate data aggregation\n",
    "*Aggregate gridded WBGT in the shade estimates into region-averaged estimates. Do this for both the reference dataset (UHE-Daily) as well as the climate change projections developed by `02_generate.ipynb`. This code is based on the Pangeo post [Conservative Region Aggregation with Xarray, Geopandas and Sparse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/1) by Ryan Abernathey. Much of the functionality is from the [extended example](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/16) by Rich Signell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import coiled\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from utils import gcm_list, load_regions, prep_sparse, spatial_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=2,\n",
    "    worker_vm_types=[\"m7g.large\"],\n",
    "    scheduler_vm_types=[\"m7g.4xlarge\"],\n",
    "    region=\"us-west-2\",\n",
    "    spot_policy=\"spot_with_fallback\",\n",
    ")\n",
    "\n",
    "\n",
    "cluster.adapt(minimum=2, maximum=50)\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Define functions for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_population(grid_name=\"CarbonPlan\"):\n",
    "    \"\"\"\n",
    "    Load the population data generated in `03_population.ipynb`.\n",
    "    \"\"\"\n",
    "    population_dict = {\n",
    "        \"CHC\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/\"\n",
    "        \"GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_UHE_daily.zarr\",\n",
    "        \"CarbonPlan\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/\"\n",
    "        \"inputs/GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_CP.zarr\",\n",
    "    }\n",
    "    population = xr.open_zarr(population_dict[grid_name])\n",
    "    population = population.rename({\"x\": \"lon\", \"y\": \"lat\"}).drop(\"spatial_ref\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ds(gcm: str, scenario: str, years: np.ndarray):\n",
    "    \"\"\"\n",
    "    Load in the gridded WBGT in the shade estimates from `02_generate.ipynb`.\n",
    "    \"\"\"\n",
    "    ds = xr.open_zarr(\n",
    "        f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-gridded/years/{gcm}/{gcm}-{scenario}.zarr\",\n",
    "        # chunks={},\n",
    "    )\n",
    "    ds = ds.sel(time=slice(str(years[0]), str(years[-1])))\n",
    "    ds = ds.assign_coords(lon=(((ds[\"lon\"] + 180) % 360) - 180)).sortby(\"lon\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee47e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-gridded/years/ACCESS-CM2/ACCESS-CM2-ssp245.zarr\",\n",
    "    # chunks={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d7f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ds(\"ACCESS-CM2\", \"historical\", np.arange(1985, 1986))  # noqa : F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62f6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ds(\"ACCESS-CM2\", \"ssp245\", np.arange(2080, 2099))  # noqa : F821\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m lat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m scenario_years \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1985\u001b[39m, \u001b[38;5;241m2015\u001b[39m),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssp245\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2015\u001b[39m, \u001b[38;5;241m2100\u001b[39m),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssp370\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m2015\u001b[39m, \u001b[38;5;241m2100\u001b[39m),\n\u001b[1;32m      7\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "lon = \"lon\"\n",
    "lat = \"lat\"\n",
    "scenario_years = {\n",
    "    \"historical\": np.arange(1985, 2015),\n",
    "    \"ssp245\": np.arange(2015, 2100),\n",
    "    \"ssp370\": np.arange(2015, 2100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "regions_df = load_regions(extension=\"central-asia\")  \n",
    "\n",
    "buffer = (\n",
    "    0.5  # padding to expand bounds to ensure you grab the data covering each region\n",
    ")\n",
    "bbox = tuple(\n",
    "    [\n",
    "        regions_df.total_bounds[0] - buffer,\n",
    "        regions_df.total_bounds[1] - buffer,\n",
    "        regions_df.total_bounds[2] + buffer,\n",
    "        regions_df.total_bounds[3] + buffer,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Access the gridded UHE-Daily data from Tuholske et al (2021) and extract timeseries for the regions of interest. These will form the reference dataset for `06_bias_correction.ipynb`. Thanks to Cascade Tuholske (Montana State University) and Pete Peterson (University of California, Santa Barbara) for making the gridded dataset available. The source gridded dataset may not remain available indefinitely, but the full city- and region-aggregated version is available here alongside the other inputs for the analysis, maintaining reproducibility of the project. \n",
    "\n",
    "The next steps aggregate the gridded datasets to region-average estimates. The non-city regions encompass all land area and thus sometimes include significant stretches of uninhabited land with potentially erroneously high or low temperatures (e.g., deserts). Weighting the aggregation by a gridded population product helps ensure that the estimates are human-relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Load the UHE-Daily dataset and calculate weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created from: https://github.com/carbonplan/uhe-daily-recipe\n",
    "ds = xr.open_zarr(\n",
    "    \"s3://carbonplan-climate-impacts/extreme-heat-extension/v1.0/inputs/uhe_daily.zarr\",\n",
    "    zarr_format=3,\n",
    "    consolidated=False,\n",
    "    chunks={},\n",
    ")\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "population = load_population(grid_name=\"CHC\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use weights to aggregate gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop = [\"WBGT\"]\n",
    "sample_time_slice = ds.isel(time=0)[\"WBGT\"].load()\n",
    "regridded = spatial_aggregation(ds, sparse_weights, load=False)\n",
    "regridded = regridded.chunk(chunks={\"time\": -1, \"processing_id\": 1000})\n",
    "logging.info(f\"{time.ctime()}: Adjusting time dtype\")\n",
    "regridded_dt = regridded.assign_coords(\n",
    "    {\"time\": regridded.time.astype(\"datetime64[ns]\")}\n",
    ")\n",
    "logging.info(f\"{time.ctime()}: Writing Zarr store\")\n",
    "fp = \"s3://carbonplan-climate-impacts/extreme-heat/v1.1/inputs/wbgt-UHE-daily-historical.zarr\"\n",
    "regridded_dt.to_zarr(fp, consolidated=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Repeat the above process but for our gridded WBGT estimates developed in `02_generate.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Load a sample dataset as a template to calculate weights. The same weights can be used for every projection because all GCMs are on the same 0.25 degree grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_ds(\"ACCESS-CM2\", \"historical\", np.arange(1985, 1986))  # noqa : F821\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "population = load_population(grid_name=\"CarbonPlan\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Aggregate all gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATE - SUBSET HERE \n",
    "for gcm in gcm_list[0:1]:\n",
    "    for scenario in [\"historical\", \"ssp245\", \"ssp370\"]:\n",
    "        logging.info(f\"Starting: {time.ctime()}: {gcm}-{scenario}\")\n",
    "        ds = load_ds(gcm, scenario, scenario_years[scenario])  # noqa : F821\n",
    "        ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "        population = population.sel(\n",
    "            lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3])\n",
    "        )\n",
    "        sample_time_slice = ds.isel(time=0)[\"WBGT\"].load()\n",
    "\n",
    "        regridded = spatial_aggregation(ds, sparse_weights, load=False)\n",
    "        regridded = regridded.chunk(chunks={\"time\": -1, \"processing_id\": 100})\n",
    "        fp = f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-regions/{gcm}-{scenario}.zarr\"\n",
    "        logging.info(f\"Writing: {time.ctime()}: {fp}\")\n",
    "        # writing to zarr version 2 b/c of the BytesBytesCodec error\n",
    "        regridded.to_zarr(fp,     zarr_format=2, mode='w',\n",
    "            consolidated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749a5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extreme-heat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
