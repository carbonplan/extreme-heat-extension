{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 05: Climate data aggregation\n",
    "*Aggregate gridded WBGT in the shade estimates into region-averaged estimates. Do this for both the reference dataset (UHE-Daily) as well as the climate change projections developed by `02_generate.ipynb`. This code is based on the Pangeo post [Conservative Region Aggregation with Xarray, Geopandas and Sparse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/1) by Ryan Abernathey. Much of the functionality is from the [extended example](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715/16) by Rich Signell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import coiled\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from utils import gcm_list, load_regions, prep_sparse, spatial_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=2,\n",
    ")\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Define functions for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_population(grid_name=\"CarbonPlan\"):\n",
    "    \"\"\"\n",
    "    Load the population data generated in `03_population.ipynb`.\n",
    "    \"\"\"\n",
    "    population_dict = {\n",
    "        \"CHC\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/\"\n",
    "        \"GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_UHE_daily.zarr\",\n",
    "        \"CarbonPlan\": \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/\"\n",
    "        \"inputs/GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_CP.zarr\",\n",
    "    }\n",
    "    population = xr.open_zarr(population_dict[grid_name])\n",
    "    population = population.rename({\"x\": \"lon\", \"y\": \"lat\"}).drop(\"spatial_ref\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_wget_strings(start_year: int, end_year: int) -> list:\n",
    "    \"\"\"\n",
    "    Access the UHE-daily gridded data product that formed the basis for\n",
    "    Tuholske et al (2021).\n",
    "    \"\"\"\n",
    "    daterange = pd.date_range(f\"{start_year}-01-01\", f\"{end_year}-12-31\")\n",
    "    return [\n",
    "        (\n",
    "            date.strftime(\"%Y.%m.%d\"),\n",
    "            f\"https://data.chc.ucsb.edu/people/cascade/UHE-daily/wbgtmax/\"\n",
    "            f\"{date.strftime('%Y')}/wbgtmax.{date.strftime('%Y.%m.%d')}.tif\",\n",
    "        )\n",
    "        for date in daterange\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_time_dim(xda):\n",
    "    \"\"\"\n",
    "    Extract datetime from tiff encoding and create time dimension when fed\n",
    "    into mfdataset.\n",
    "    \"\"\"\n",
    "    xda = xda.expand_dims(\n",
    "        time=[np.datetime64(\"-\".join(xda.encoding[\"source\"].split(\".\")[-4:-1]))]\n",
    "    )\n",
    "    return xda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_uhedaily_ds(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Load in the UHE-daily dataset.\n",
    "    \"\"\"\n",
    "    strings = gen_wget_strings(start_year, end_year)\n",
    "    subset = [i[1] for i in strings]\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            subset,\n",
    "            engine=\"rasterio\",\n",
    "            chunks={},\n",
    "            parallel=True,\n",
    "            concat_dim=\"time\",\n",
    "            combine=\"nested\",\n",
    "            preprocess=add_time_dim,\n",
    "        )\n",
    "        .squeeze(dim=[\"band\"], drop=True)\n",
    "        .drop(\"spatial_ref\")\n",
    "        .rename({\"band_data\": \"WBGT\", \"x\": \"lon\", \"y\": \"lat\"})\n",
    "        .sortby(\"lat\")\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ds(gcm, scenario, years):\n",
    "    \"\"\"\n",
    "    Load in the gridded WBGT in the shade estimates from `02_generate.ipynb`.\n",
    "    \"\"\"\n",
    "    with dask.config.set(**{\"array.slicing.split_large_chunks\": False}):\n",
    "        stores = [\n",
    "            f's3://carbonplan-scratch/extreme-heat/wbgt-shade-gridded/years/{gcm}/{gcm}-{scenario.split('-')[0]}-{year}.zarr'\n",
    "            for year in years\n",
    "        ]\n",
    "        ds = xr.open_mfdataset(stores, engine=\"zarr\", chunks={})\n",
    "        ds = ds.assign_coords(lon=(((ds[lon] + 180) % 360) - 180)).sortby(lon)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lon = \"lon\"\n",
    "lat = \"lat\"\n",
    "scenario_years = {\n",
    "    \"historical\": np.arange(1985, 2015),\n",
    "    \"ssp245\": np.arange(2015, 2091),\n",
    "    \"ssp370\": np.arange(2015, 2091),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regions_df = load_regions(extension=True)\n",
    "# use an area-preserving projection\n",
    "buffer = (\n",
    "    0.5  # padding to expand bounds to ensure you grab the data covering each region\n",
    ")\n",
    "bbox = tuple(\n",
    "    [\n",
    "        regions_df.total_bounds[0] - buffer,\n",
    "        regions_df.total_bounds[1] - buffer,\n",
    "        regions_df.total_bounds[2] + buffer,\n",
    "        regions_df.total_bounds[3] + buffer,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Access the gridded UHE-Daily data from Tuholske et al (2021) and extract timeseries for the regions of interest. These will form the reference dataset for `06_bias_correction.ipynb`. Thanks to Cascade Tuholske (Montana State University) and Pete Peterson (University of California, Santa Barbara) for making the gridded dataset available. The source gridded dataset may not remain available indefinitely, but the full city- and region-aggregated version is available here alongside the other inputs for the analysis, maintaining reproducibility of the project. \n",
    "\n",
    "The next steps aggregate the gridded datasets to region-average estimates. The non-city regions encompass all land area and thus sometimes include significant stretches of uninhabited land with potentially erroneously high or low temperatures (e.g., deserts). Weighting the aggregation by a gridded population product helps ensure that the estimates are human-relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Load the UHE-Daily dataset and calculate weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_uhedaily_ds(1983, 2016)\n",
    "fp = \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/wbgt-UHE-daily-historical.zarr\"\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "population = load_population(grid_name=\"CHC\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use weights to aggregate gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop = [\"WBGT\"]\n",
    "sample_time_slice = ds.isel(time=0)[\"WBGT\"].load()\n",
    "regridded = spatial_aggregation(ds, sparse_weights, \"processing_id\", load=False)\n",
    "regridded = regridded.chunk(chunks={\"time\": -1, \"processing_id\": 1000})\n",
    "logging.info(f\"{time.ctime()}: Adjusting time dtype\")\n",
    "regridded_dt = regridded.assign_coords(\n",
    "    {\"time\": regridded.time.astype(\"datetime64[ns]\")}\n",
    ")\n",
    "logging.info(f\"{time.ctime()}: Writing Zarr store\")\n",
    "regridded_dt.to_zarr(fp, consolidated=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Repeat the above process but for our gridded WBGT estimates developed in `02_generate.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Load a sample dataset as a template to calculate weights. The same weights can be used for every projection because all GCMs are on the same 0.25 degree grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_ds(\"ACCESS-CM2\", \"historical\", np.arange(1985, 1986))  # noqa : F821\n",
    "ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "population = load_population(grid_name=\"CarbonPlan\")\n",
    "population = population.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "sparse_weights, population = prep_sparse(\n",
    "    ds, population, regions_df, return_population=True, variables_to_drop=[\"WBGT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Aggregate all gridded estimates into region-average estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for gcm in gcm_list:\n",
    "    for scenario in [\"historical\", \"ssp245\", \"ssp370\"]:\n",
    "        logging.info(f\"Starting: {time.ctime()}: {gcm}-{scenario}\")\n",
    "        ds = load_ds(gcm, scenario, scenario_years[scenario])  # noqa : F821\n",
    "        ds = ds.sel(lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3]))\n",
    "        population = population.sel(\n",
    "            lon=slice(bbox[0], bbox[2]), lat=slice(bbox[1], bbox[3])\n",
    "        )\n",
    "        sample_time_slice = ds.isel(time=0)[\"WBGT\"].load()\n",
    "\n",
    "        regridded = spatial_aggregation(ds, sparse_weights, \"processing_id\", load=False)\n",
    "        regridded = regridded.chunk(chunks={\"time\": -1, \"processing_id\": 100})\n",
    "        fp = f\"s3://carbonplan-scratch/extreme-heat/wbgt-shade-regions/{gcm}-{scenario}.zarr\"\n",
    "        logging.info(f\"Writing: {time.ctime()}: {fp}\")\n",
    "        regridded.to_zarr(fp, consolidated=True, mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
