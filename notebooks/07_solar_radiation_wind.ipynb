{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 07: Solar radiation and wind data aggregation\n",
    "*Extract daily solar radiation and wind data for subsequent use in developing WBGT in the sun estimates in `08_shade_sun_adjustment.ipynb`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from metsim.datetime import date_range\n",
    "from metsim.disaggregate import shortwave\n",
    "from metsim.physics import solar_geom\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    gcm_list,\n",
    "    gcms_with_nonstandard_calendars_list,\n",
    "    load_nasanex,\n",
    "    load_regions,\n",
    "    prep_sparse,\n",
    "    remove_360_longitudes,\n",
    "    spatial_aggregation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Set up cluster to handle multiprocessing using a Dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(n_workers=70, worker_memory=\"8 GiB\")\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_years = [\n",
    "    (\"historical\", np.arange(1985, 2015)),\n",
    "    (\"ssp370\", np.arange(2015, 2081)),\n",
    "    (\"ssp245\", np.arange(2015, 2081)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df = load_regions(extension='se-europe')\n",
    "buffer = (\n",
    "    0.5  # padding to expand bounds to ensure you grab the data covering each region\n",
    ")\n",
    "bbox = tuple(\n",
    "    [\n",
    "        regions_df.total_bounds[0] - buffer,\n",
    "        regions_df.total_bounds[1] - buffer,\n",
    "        regions_df.total_bounds[2] + buffer,\n",
    "        regions_df.total_bounds[3] + buffer,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "As in `05_aggregate.ipynb`, aggregate the solar radiation and wind information into population-weighted region averages. Standardize calendars as was done in `06_bias_correction.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = load_nasanex(\n",
    "    \"historical\", \"ACCESS-CM2\", [\"rsds\", \"sfcWind\"], np.arange(1990, 1991)\n",
    ")\n",
    "population = xr.open_zarr(\n",
    "    \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/GHS_POP_E2030_GLOBE_R2023A_4326_30ss_V1_0_resampled_to_CP.zarr\"\n",
    ")\n",
    "population = population.rename({\"x\": \"lon\", \"y\": \"lat\"}).drop(\"spatial_ref\")\n",
    "# Calculate sparse weights once to use for all of the data files.\n",
    "sparse_weights, population = prep_sparse(\n",
    "    sample_ds, population, return_population=True, variables_to_drop=[\"rsds\", \"sfcWind\"]\n",
    ")\n",
    "for gcm in gcm_list:\n",
    "    # loop through the gcms and extract the data for each region\n",
    "    extracted_lists = {\"ssp245\": [], \"ssp370\": []}\n",
    "    for scenario, years in scenario_years:\n",
    "        wind_solrad = load_nasanex(scenario, gcm, [\"rsds\", \"sfcWind\"], years)\n",
    "        wind_solrad = remove_360_longitudes(wind_solrad)\n",
    "\n",
    "        # ensure population matches the same lon coords order\n",
    "        assert (population[\"lon\"].values == wind_solrad[\"lon\"].values).all()\n",
    "        assert (population[\"lat\"].values == wind_solrad[\"lat\"].values).all()\n",
    "        assert (\n",
    "            population[\"population\"].values.shape\n",
    "            == wind_solrad[\"rsds\"].isel(time=0).values.shape\n",
    "        )\n",
    "        # aggregate the wind_solrad to regional estimates\n",
    "        extracted_wind_solrad = spatial_aggregation(wind_solrad, sparse_weights)\n",
    "        if scenario == \"historical\":\n",
    "            extracted_lists[\"ssp245\"].append(extracted_wind_solrad)\n",
    "            extracted_lists[\"ssp370\"].append(extracted_wind_solrad)\n",
    "        else:\n",
    "            extracted_lists[scenario].append(extracted_wind_solrad)\n",
    "        if scenario in [\"ssp245\", \"ssp370\"]:\n",
    "            extracted_wind_solrad_ds = xr.concat(extracted_lists[scenario], dim=\"time\")\n",
    "            if gcm in gcms_with_nonstandard_calendars_list:\n",
    "                # convert to standard calendar by filling with nans\n",
    "                extracted_wind_solrad_ds = extracted_wind_solrad_ds.convert_calendar(\n",
    "                    \"gregorian\",\n",
    "                    dim=\"time\",\n",
    "                    align_on=\"year\",\n",
    "                    missing=np.nan,\n",
    "                    use_cftime=None,\n",
    "                )\n",
    "                # gap fill by linearly interpolating\n",
    "                extracted_wind_solrad_ds = extracted_wind_solrad_ds.interpolate_na(\n",
    "                    dim=\"time\", method=\"linear\"\n",
    "                )\n",
    "            output = f\"s3://carbonplan-scratch/extreme-heat/wind_solrad-regions/{gcm}-{scenario}-wind-solrad-regions.zarr\"\n",
    "            t = extracted_wind_solrad_ds.to_zarr(\n",
    "                output, consolidated=True, mode=\"w\", compute=False\n",
    "            )\n",
    "            t = dask.optimize(t)[0]\n",
    "            t.compute(retries=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Calculate representative elevation and latitude for each region, which will be used below by `metsim` for solar geometry calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev = xr.open_zarr(\n",
    "    \"s3://carbonplan-climate-impacts/extreme-heat/v1.0/inputs/elevation.zarr\"\n",
    ")\n",
    "elev = elev.chunk({\"lat\": -1, \"lon\": -1}).compute()\n",
    "\n",
    "sparse_weights = prep_sparse(sample_ds, population, regions_df, return_population=False)\n",
    "elev = remove_360_longitudes(elev)\n",
    "# attach a placeholder time timension\n",
    "elev = elev.expand_dims(dim=\"time\").assign_coords(\n",
    "    {\"time\": pd.date_range(\"2000-01-01\", \"2000-01-01\")}\n",
    ")\n",
    "assert (population[\"lon\"].values == elev[\"lon\"].values).all()\n",
    "assert (population[\"lat\"].values == elev[\"lat\"].values).all()\n",
    "assert (\n",
    "    population[\"population\"].values.shape == elev[\"elevation\"].isel(time=0).values.shape\n",
    ")\n",
    "elev_regions = spatial_aggregation(elev, sparse_weights).drop(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_ds = xr.Dataset(\n",
    "    {\n",
    "        \"latitude\": xr.DataArray(\n",
    "            data=np.tile(elev.lat.values, (len(elev.lon.values), 1)).transpose(),\n",
    "            coords={\"lat\": elev.lat.values, \"lon\": elev.lon.values},\n",
    "        )\n",
    "    }\n",
    ")\n",
    "# attach a placeholder time dimension\n",
    "lat_ds = lat_ds.expand_dims(dim=\"time\").assign_coords(\n",
    "    {\"time\": pd.date_range(\"2000-01-01\", \"2000-01-01\")}\n",
    ")\n",
    "assert (population[\"lon\"].values == lat_ds[\"lon\"].values).all()\n",
    "assert (population[\"lat\"].values == lat_ds[\"lat\"].values).all()\n",
    "assert (\n",
    "    population[\"population\"].values.shape\n",
    "    == lat_ds[\"latitude\"].isel(time=0).values.shape\n",
    ")\n",
    "lat_regions = spatial_aggregation(lat_ds, sparse_weights).drop(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Take advantage of utilities in the `metsim` package, developed largely by Andrew Bennett at the University of Arizona. The `solar_geom`, `shortwave`, and `date_range` functions are slightly different from their implementations in the `metsim` package for this use case which focuses solely on solar radiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "For each region, use elevation and latitude information calculated above to derive radiation parameters like day length and subdaily maximum solar radiation. This calculation only needs to be done once because, while it varies in time throughout the year for every location, it will be the same for every projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"time_step\": 60,\n",
    "    \"method\": \"other\",\n",
    "    \"utc_offset\": False,\n",
    "    \"calendar\": \"gregorian\",\n",
    "    \"SW_RAD_DT\": 3600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_rad_fract_list, daylength_list = [], []\n",
    "for processing_id in tqdm(elev_regions.processing_id.values):\n",
    "    sg = solar_geom(\n",
    "        elev_regions.sel(processing_id=processing_id)[\"elevation\"].values[0],\n",
    "        lat_regions.sel(processing_id=processing_id)[\"latitude\"].values[0],\n",
    "        -6.5,\n",
    "        params,\n",
    "    )\n",
    "    tiny_rad_fract_list.append(\n",
    "        xr.DataArray(data=sg[0], dims=(\"dayofyear\", \"tiny_timestep\"))\n",
    "    )\n",
    "    daylength_list.append(xr.DataArray(data=sg[1], dims=(\"dayofyear\")))\n",
    "radiation_parameters = xr.Dataset(\n",
    "    {\n",
    "        \"tiny_rad_fract\": xr.concat(tiny_rad_fract_list, dim=\"processing_id\"),\n",
    "        \"daylength\": xr.concat(daylength_list, dim=\"processing_id\"),\n",
    "    }\n",
    ")\n",
    "radiation_parameters = radiation_parameters.assign_coords(\n",
    "    {\"processing_id\": elev_regions.processing_id.values}\n",
    ")\n",
    "\n",
    "radiation_parameters = radiation_parameters.chunk(\n",
    "    {\"dayofyear\": -1, \"tiny_timestep\": -1, \"processing_id\": 4000}\n",
    ")\n",
    "radiation_parameters.to_zarr(\n",
    "    \"s3://carbonplan-scratch/extreme-heat/wind_solrad-regions/radiation_parameters.zarr\",\n",
    "    mode=\"w\",\n",
    ")\n",
    "radiation_parameters = xr.open_zarr(\n",
    "    \"s3://carbonplan-scratch/extreme-heat/wind_solrad-regions/radiation_parameters.zarr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Set up a dataframe template to store the data and functions for calculating maximum daily solar radiation from the daily mean solar radiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_daily_template = pd.DataFrame(index=pd.date_range(\"1985-01-01\", \"2080-12-31\"))\n",
    "stop = (\n",
    "    df_daily_template.index[-1]\n",
    "    + pd.Timedelta(\"1 days\")\n",
    "    - pd.Timedelta(\"{} minutes\".format(params[\"time_step\"]))\n",
    ")\n",
    "dates_disagg = date_range(\n",
    "    df_daily_template.index[0],\n",
    "    stop,\n",
    "    freq=\"{}T\".format(params[\"time_step\"]),\n",
    "    calendar=params[\"calendar\"],\n",
    ")\n",
    "df_disagg_template = pd.DataFrame(index=dates_disagg)\n",
    "yday = df_daily_template.index.dayofyear - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortwave_wrapper(rsds, daylengths, tiny_rad_fract):\n",
    "    \"\"\"\n",
    "    Wrapper function for shortwave which supports vectorized computation\n",
    "    via `xr.ufunc`\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"time_step\": 60,\n",
    "        \"method\": \"other\",\n",
    "        \"utc_offset\": False,\n",
    "        \"calendar\": \"gregorian\",\n",
    "        \"SW_RAD_DT\": 3600,\n",
    "    }\n",
    "    dayofyear = pd.date_range(\"1985-01-01\", \"2080-12-31\").dayofyear.values\n",
    "    shortwave_out = shortwave(rsds, daylengths[yday], dayofyear, tiny_rad_fract, params)\n",
    "    da = xr.DataArray(shortwave_out, dims=[\"hourlytime\"])\n",
    "    da = da.assign_coords(\n",
    "        {\n",
    "            \"hourlytime\": pd.date_range(\n",
    "                \"1985-01-01 00:00:00\", \"2080-12-31 23:00:00\", freq=\"H\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    output = da.resample({\"hourlytime\": \"D\"}).max().data\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Calculate maximum solar radiation given daily mean solar radiation and radiation parameters (as calculated above). This approach accounts for the cooling effect of clouds but does not capture subdaily variations in cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiation_parameters = radiation_parameters.chunk(\n",
    "    {\"processing_id\": 25, \"tiny_timestep\": -1, \"dayofyear\": -1}\n",
    ")\n",
    "for gcm in gcm_list:\n",
    "    wind_solrad_ds = xr.open_zarr(\n",
    "        f\"s3://carbonplan-scratch/extreme-heat/wind_solrad-regions/{gcm}-{scenario}-wind-solrad-regions.zarr\"\n",
    "    ).persist()\n",
    "    wind_solrad_ds = wind_solrad_ds.chunk({\"processing_id\": 25, \"time\": -1})\n",
    "\n",
    "    max_solrad = xr.apply_ufunc(\n",
    "        shortwave_wrapper,\n",
    "        wind_solrad_ds[\"rsds\"],\n",
    "        radiation_parameters.daylength,\n",
    "        radiation_parameters.tiny_rad_fract,\n",
    "        input_core_dims=[[\"time\"], [\"dayofyear\"], [\"dayofyear\", \"tiny_timestep\"]],\n",
    "        output_core_dims=[[\"time\"]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[wind_solrad_ds[\"rsds\"].dtype],\n",
    "    )\n",
    "\n",
    "    out_store = f\"s3://carbonplan-scratch/extreme-heat/wind_solrad-regions/{gcm}-{scenario}-rsds-max-regions.zarr\"\n",
    "    max_solrad.to_zarr(out_store, mode=\"w\", consolidated=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
